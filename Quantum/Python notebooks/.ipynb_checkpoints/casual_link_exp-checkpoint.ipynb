{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ae7f4e-1653-4a20-b1ec-b0403ae7cfdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viola Experiment 7.0: Classical Grammatical Parser Benchmark\n",
      "\n",
      "Loaded spaCy model 'en_core_web_sm'.\n",
      "\n",
      "[Phase 1: Evaluating Classical Parser on Ambiguous Sentences]\n",
      "\n",
      "Sentence: 'I saw the man with the telescope.'\n",
      "  - Correct Interpretation (1): I saw a man who was holding a telescope.\n",
      "  - spaCy Predicted (1): I saw a man who was holding a telescope.\n",
      "\n",
      "Sentence: 'The dog chased the cat in the garden.'\n",
      "  - Correct Interpretation (1): The cat was in the garden when it was chased.\n",
      "  - spaCy Predicted (0): The dog was in the garden when it chased the cat.\n",
      "\n",
      "Sentence: 'We painted the wall with cracks.'\n",
      "  - Correct Interpretation (1): We painted the wall that already had cracks.\n",
      "  - spaCy Predicted (0): We used paint that had cracks in it to paint the wall.\n",
      "\n",
      "Sentence: 'Sherlock saw the suspect with binoculars.'\n",
      "  - Correct Interpretation (0): Sherlock used binoculars to see the suspect.\n",
      "  - spaCy Predicted (0): Sherlock used binoculars to see the suspect.\n",
      "\n",
      "Sentence: 'The company reported a loss for the last quarter.'\n",
      "  - Correct Interpretation (0): The company reported a loss that occurred during the last quarter.\n",
      "  - spaCy Predicted (1): The company used the last quarter of the year to report a loss.\n",
      "\n",
      "==================================================\n",
      "      VIOLA 7.0: FINAL CLASSICAL BENCHMARK       \n",
      "==================================================\n",
      "Overall Accuracy: 40.00%\n",
      "Weighted F1-Score: 40.00%\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Interpretation 1       0.33      0.50      0.40         2\n",
      "Interpretation 2       0.50      0.33      0.40         3\n",
      "\n",
      "        accuracy                           0.40         5\n",
      "       macro avg       0.42      0.42      0.40         5\n",
      "    weighted avg       0.43      0.40      0.40         5\n",
      "\n",
      "\n",
      "Benchmark established. The classical parser is imperfect on this task.\n",
      "This creates a clear opportunity for a QNLP model to demonstrate an advantage.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "# Ensure you have the spaCy model downloaded:\n",
    "# python -m spacy download en_core_web_sm\n",
    "SPACY_MODEL_NAME = \"en_core_web_sm\"\n",
    "\n",
    "# --- Part 1: The Ambiguous Dataset ---\n",
    "\n",
    "# The task is to identify the correct interpretation of a sentence.\n",
    "# Label 0: Interpretation 1\n",
    "# Label 1: Interpretation 2\n",
    "dataset = [\n",
    "    {\n",
    "        \"sentence\": \"I saw the man with the telescope.\",\n",
    "        \"interpretation_1\": \"I used a telescope to see the man.\",\n",
    "        \"interpretation_2\": \"I saw a man who was holding a telescope.\",\n",
    "        \"correct_label\": 1 # The more common interpretation\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The dog chased the cat in the garden.\",\n",
    "        \"interpretation_1\": \"The dog was in the garden when it chased the cat.\",\n",
    "        \"interpretation_2\": \"The cat was in the garden when it was chased.\",\n",
    "        \"correct_label\": 1 # The prepositional phrase attaches to the object\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"We painted the wall with cracks.\",\n",
    "        \"interpretation_1\": \"We used paint that had cracks in it to paint the wall.\",\n",
    "        \"interpretation_2\": \"We painted the wall that already had cracks.\",\n",
    "        \"correct_label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Sherlock saw the suspect with binoculars.\",\n",
    "        \"interpretation_1\": \"Sherlock used binoculars to see the suspect.\",\n",
    "        \"interpretation_2\": \"The suspect was carrying binoculars.\",\n",
    "        \"correct_label\": 0 # Here, the instrument interpretation is more likely\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company reported a loss for the last quarter.\",\n",
    "        \"interpretation_1\": \"The company reported a loss that occurred during the last quarter.\",\n",
    "        \"interpretation_2\": \"The company used the last quarter of the year to report a loss.\",\n",
    "        \"correct_label\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Part 2: Classical Parsing Logic ---\n",
    "\n",
    "def classify_interpretation_with_spacy(nlp, sentence_text):\n",
    "    \"\"\"\n",
    "    Uses spaCy's dependency parser to classify the sentence structure.\n",
    "    This is a heuristic-based approach.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence_text)\n",
    "    \n",
    "    # Specific heuristic for \"I saw the man with the telescope.\"\n",
    "    if \"saw\" in sentence_text and \"with\" in sentence_text:\n",
    "        for token in doc:\n",
    "            if token.text == \"with\":\n",
    "                # Check if \"with\" is attached to the verb (\"saw\") or the noun (\"man\")\n",
    "                if token.head.text == \"saw\":\n",
    "                    return 0 # Interpretation 1: \"saw with telescope\"\n",
    "                elif token.head.text == \"man\":\n",
    "                    return 1 # Interpretation 2: \"man with telescope\"\n",
    "\n",
    "    # Specific heuristic for \"The dog chased the cat in the garden.\"\n",
    "    if \"chased\" in sentence_text and \"in\" in sentence_text:\n",
    "        for token in doc:\n",
    "            if token.text == \"in\":\n",
    "                if token.head.text == \"chased\":\n",
    "                    return 0 # Interpretation 1: \"chased in the garden\"\n",
    "                elif token.head.text == \"cat\":\n",
    "                    return 1 # Interpretation 2: \"cat in the garden\"\n",
    "\n",
    "    # Specific heuristic for \"We painted the wall with cracks.\"\n",
    "    if \"painted\" in sentence_text and \"with\" in sentence_text:\n",
    "        for token in doc:\n",
    "            if token.text == \"with\":\n",
    "                if token.head.text == \"painted\":\n",
    "                     return 0 # Interpretation 1: \"painted with cracks\"\n",
    "                elif token.head.text == \"wall\":\n",
    "                     return 1 # Interpretation 2: \"wall with cracks\"\n",
    "    \n",
    "    # Default fallback if no specific heuristic matches\n",
    "    # In a real system, more rules would be needed. For this benchmark,\n",
    "    # we assume the parser's default for unhandled cases might be incorrect.\n",
    "    # For the remaining sentences, we'll check the verb's prepositional modifier.\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"prep\" and token.head.pos_ == \"VERB\":\n",
    "             return 0 # Assume attachment to the verb if no other rule applies\n",
    "    \n",
    "    return 1 # Default to the object attachment otherwise\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Viola Experiment 7.0: Classical Grammatical Parser Benchmark\")\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(SPACY_MODEL_NAME)\n",
    "        print(f\"\\nLoaded spaCy model '{SPACY_MODEL_NAME}'.\")\n",
    "    except OSError:\n",
    "        print(f\"spaCy model '{SPACY_MODEL_NAME}' not found.\")\n",
    "        print(f\"Please run: python -m spacy download {SPACY_MODEL_NAME}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n[Phase 1: Evaluating Classical Parser on Ambiguous Sentences]\")\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for item in dataset:\n",
    "        sentence = item[\"sentence\"]\n",
    "        correct_label = item[\"correct_label\"]\n",
    "        \n",
    "        predicted_label = classify_interpretation_with_spacy(nlp, sentence)\n",
    "        \n",
    "        true_labels.append(correct_label)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        \n",
    "        print(f\"\\nSentence: '{sentence}'\")\n",
    "        print(f\"  - Correct Interpretation ({correct_label}): {item[f'interpretation_{correct_label+1}']}\")\n",
    "        print(f\"  - spaCy Predicted ({predicted_label}): {item[f'interpretation_{predicted_label+1}']}\")\n",
    "\n",
    "\n",
    "    # --- Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      VIOLA 7.0: FINAL CLASSICAL BENCHMARK       \")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Weighted F1-Score: {f1:.2%}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels, target_names=['Interpretation 1', 'Interpretation 2']))\n",
    "    \n",
    "    if accuracy < 1.0:\n",
    "        print(\"\\nBenchmark established. The classical parser is imperfect on this task.\")\n",
    "        print(\"This creates a clear opportunity for a QNLP model to demonstrate an advantage.\")\n",
    "    else:\n",
    "        print(\"\\nBenchmark established. The classical parser achieved a perfect score.\")\n",
    "        print(\"The dataset may need to be expanded with more challenging ambiguities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d201259-bfc0-4f59-85ce-f570f6dc0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambeq library not found or has an unexpected structure. Please ensure you have the latest version: pip install -U lambeq\n",
      "Viola Experiment 7.0: QNLP Quantum Implementation (Hardware Execution)\n",
      "\n",
      "[Phase 1: Parsing sentences and generating quantum circuits...]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'product'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# 1. Create the QNLP circuits\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Phase 1: Parsing sentences and generating quantum circuits...]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m all_circuits \u001b[38;5;241m=\u001b[39m create_qnlp_circuits(sentences)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_circuits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m quantum circuits from the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 2. Initialize Quantum Backend and Model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m, in \u001b[0;36mcreate_qnlp_circuits\u001b[1;34m(sentence_list)\u001b[0m\n\u001b[0;32m     81\u001b[0m S \u001b[38;5;241m=\u001b[39m AtomicType\u001b[38;5;241m.\u001b[39mSENTENCE\n\u001b[0;32m     82\u001b[0m ansatz \u001b[38;5;241m=\u001b[39m SpiderAnsatz({N: \u001b[38;5;241m1\u001b[39m, S: \u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m---> 84\u001b[0m train_circuits \u001b[38;5;241m=\u001b[39m [ansatz(diagram) \u001b[38;5;28;01mfor\u001b[39;00m diagram \u001b[38;5;129;01min\u001b[39;00m diagrams]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_circuits\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\ansatz\\tensor.py:153\u001b[0m, in \u001b[0;36mSplitTensorAnsatz.__call__\u001b[1;34m(self, diagram)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diagram\u001b[38;5;241m.\u001b[39mhas_frames:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AnsatzWithFramesRuntimeError\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctor(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_functor(diagram)\n\u001b[0;32m    155\u001b[0m )\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:2046\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[1;34m(self, entity)\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(entity)\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mar_with_cache(entity)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:2077\u001b[0m, in \u001b[0;36mFunctor.ar_with_cache\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mis_id:\n\u001b[1;32m-> 2077\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mapply_functor(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2079\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mDiagram\u001b[38;5;241m.\u001b[39mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(ar\u001b[38;5;241m.\u001b[39mdom))\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1325\u001b[0m, in \u001b[0;36mDiagram.apply_functor\u001b[1;34m(self, functor)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m   1323\u001b[0m     left, box, right \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39munpack()\n\u001b[0;32m   1324\u001b[0m     diagram \u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m=\u001b[39m (functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid(left))\n\u001b[1;32m-> 1325\u001b[0m                  \u001b[38;5;241m@\u001b[39m functor(box)\u001b[38;5;241m.\u001b[39mto_diagram()\n\u001b[0;32m   1326\u001b[0m                  \u001b[38;5;241m@\u001b[39m functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid(right)))\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diagram\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:2046\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[1;34m(self, entity)\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(entity)\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mar_with_cache(entity)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:2077\u001b[0m, in \u001b[0;36mFunctor.ar_with_cache\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mis_id:\n\u001b[1;32m-> 2077\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mapply_functor(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2079\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mDiagram\u001b[38;5;241m.\u001b[39mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(ar\u001b[38;5;241m.\u001b[39mdom))\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:531\u001b[0m, in \u001b[0;36mBox.apply_functor\u001b[1;34m(self, functor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwind())\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functor\u001b[38;5;241m.\u001b[39mar(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\backend\\grammar.py:2105\u001b[0m, in \u001b[0;36mFunctor.ar\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecify a custom ar function if you want to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2103\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse the functor on boxes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ar(\u001b[38;5;28mself\u001b[39m, ar)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\lambeq\\ansatz\\tensor.py:60\u001b[0m, in \u001b[0;36mTensorAnsatz._ar\u001b[1;34m(self, functor, box)\u001b[0m\n\u001b[0;32m     55\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarise_box(box)\n\u001b[0;32m     57\u001b[0m directed_dom, directed_cod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_directed_dom_cod(box)\n\u001b[0;32m     58\u001b[0m syms \u001b[38;5;241m=\u001b[39m Symbol(name,\n\u001b[0;32m     59\u001b[0m               directed_dom\u001b[38;5;241m=\u001b[39mdirected_dom\u001b[38;5;241m.\u001b[39mproduct,\n\u001b[1;32m---> 60\u001b[0m               directed_cod\u001b[38;5;241m=\u001b[39mdirected_cod\u001b[38;5;241m.\u001b[39mproduct)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Box domain and codomain are unchanged\u001b[39;00m\n\u001b[0;32m     63\u001b[0m dom \u001b[38;5;241m=\u001b[39m functor(box\u001b[38;5;241m.\u001b[39mdom)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'product'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- QNLP and Qiskit Imports ---\n",
    "# Ensure you have lambeq installed: pip install lambeq\n",
    "try:\n",
    "    from lambeq import (\n",
    "        BobcatParser,\n",
    "        SpiderAnsatz,\n",
    "        AtomicType,\n",
    "        Discard,\n",
    "        QuantumTrainer,\n",
    "        QiskitModel\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"lambeq library not found or has an unexpected structure. Please ensure you have the latest version: pip install -U lambeq\")\n",
    "    exit()\n",
    "\n",
    "# --- Qiskit Runtime Imports for Hardware Execution ---\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "BACKEND_NAME = \"ibm_brisbane\" # Target real quantum hardware\n",
    "\n",
    "# --- Part 1: The Ambiguous Dataset (Identical to Classical Benchmark) ---\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        \"sentence\": \"I saw the man with the telescope.\",\n",
    "        \"interpretation_1\": \"I used a telescope to see the man.\",\n",
    "        \"interpretation_2\": \"I saw a man who was holding a telescope.\",\n",
    "        \"correct_label\": 1 \n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The dog chased the cat in the garden.\",\n",
    "        \"interpretation_1\": \"The dog was in the garden when it chased the cat.\",\n",
    "        \"interpretation_2\": \"The cat was in the garden when it was chased.\",\n",
    "        \"correct_label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"We painted the wall with cracks.\",\n",
    "        \"interpretation_1\": \"We used paint that had cracks in it to paint the wall.\",\n",
    "        \"interpretation_2\": \"We painted the wall that already had cracks.\",\n",
    "        \"correct_label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Sherlock saw the suspect with binoculars.\",\n",
    "        \"interpretation_1\": \"Sherlock used binoculars to see the suspect.\",\n",
    "        \"interpretation_2\": \"The suspect was carrying binoculars.\",\n",
    "        \"correct_label\": 0\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company reported a loss for the last quarter.\",\n",
    "        \"interpretation_1\": \"The company reported a loss that occurred during the last quarter.\",\n",
    "        \"interpretation_2\": \"The company used the last quarter of the year to report a loss.\",\n",
    "        \"correct_label\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "sentences = [item[\"sentence\"] for item in dataset]\n",
    "labels = [item[\"correct_label\"] for item in dataset]\n",
    "y_true = np.array(labels)\n",
    "\n",
    "# --- Part 2: The QNLP Pipeline ---\n",
    "\n",
    "def create_qnlp_circuits(sentence_list):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of trainable Qiskit circuits.\n",
    "    \"\"\"\n",
    "    parser = BobcatParser(verbose='suppress')\n",
    "    diagrams = parser.sentences2diagrams(sentence_list)\n",
    "    \n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "    ansatz = SpiderAnsatz({N: 1, S: 1})\n",
    "\n",
    "    train_circuits = [ansatz(diagram) for diagram in diagrams]\n",
    "    return train_circuits\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Viola Experiment 7.0: QNLP Quantum Implementation (Hardware Execution)\")\n",
    "    \n",
    "    # 1. Create the QNLP circuits\n",
    "    print(\"\\n[Phase 1: Parsing sentences and generating quantum circuits...]\")\n",
    "    all_circuits = create_qnlp_circuits(sentences)\n",
    "    print(f\"Successfully generated {len(all_circuits)} quantum circuits from the dataset.\")\n",
    "\n",
    "    # 2. Initialize Quantum Backend and Model\n",
    "    print(\"\\n[Phase 2: Initializing IBM Quantum Hardware and QNLP Model...]\")\n",
    "    \n",
    "    # Load IBM Quantum credentials from .env file or environment variables\n",
    "    load_dotenv()\n",
    "    # IMPORTANT: Ensure you have a .env file with your IBM_QUANTUM_TOKEN\n",
    "    token = \"LqGVfcxumYuwWrCM6wPUEy2gkmPHMdbDtgRWPNAxdDUK\"\n",
    "    if not token:\n",
    "        print(\"Error: IBM_QUANTUM_TOKEN not found. Please create a .env file or set the environment variable.\")\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        service = QiskitRuntimeService(channel=\"ibm_quantum_platform\", token=token, instance=\"test\")\n",
    "        backend = service.backend(BACKEND_NAME)\n",
    "        print(f\"Successfully connected to backend: {BACKEND_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to IBM Quantum: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # The QiskitModel from lambeq is compatible with Qiskit Runtime backends\n",
    "    model = QiskitModel.from_diagrams(all_circuits, backend_device=backend)\n",
    "    \n",
    "    # 3. Initialize Trainer\n",
    "    trainer = QuantumTrainer(\n",
    "        model,\n",
    "        loss_function=lambda y_hat, y: -y_hat[y.argmax()],\n",
    "        epochs=100,\n",
    "        optimizer_kwargs={'lr': 0.1},\n",
    "        verbose='text',\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # 4. Train the QNLP model\n",
    "    print(\"\\n[Phase 3: Training the QNLP model on hardware...]\")\n",
    "    y_train_one_hot = np.eye(2)[y_true]\n",
    "    trainer.fit(all_circuits, y_train_one_hot)\n",
    "\n",
    "    # 5. Evaluate the trained model\n",
    "    print(\"\\n[Phase 4: Evaluating the trained QNLP model...]\")\n",
    "    y_pred_probs = model(all_circuits)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # --- Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"         VIOLA 7.0: FINAL QUANTUM RESULTS         \")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Weighted F1-Score: {f1:.2%}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Interpretation 1', 'Interpretation 2']))\n",
    "\n",
    "    classical_f1 = 0.40  # From our classical benchmark\n",
    "    if f1 > classical_f1:\n",
    "        print(\"\\nSUCCESS: The QNLP model outperformed the classical parser.\")\n",
    "        print(\"This represents a 'viola moment' for quantum advantage in this task.\")\n",
    "    else:\n",
    "        print(\"\\nRESULT: The QNLP model did not outperform the classical parser.\")\n",
    "        print(\"Further tuning of the QNLP ansatz or training may be required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a5bf4e-d71e-4486-b3fa-17f2ce5ada73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting depccg\n",
      "  Using cached depccg-2.0.3.2.tar.gz (3.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting simplejson (from depccg)\n",
      "  Downloading simplejson-3.20.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: cython in c:\\anaconda3\\lib\\site-packages (from depccg) (3.1.3)\n",
      "Requirement already satisfied: lxml in c:\\anaconda3\\lib\\site-packages (from depccg) (5.2.1)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda3\\lib\\site-packages (from depccg) (6.0.2)\n",
      "Requirement already satisfied: nltk in c:\\anaconda3\\lib\\site-packages (from depccg) (3.9.1)\n",
      "Collecting chainer>=2.0.0 (from depccg)\n",
      "  Downloading chainer-7.8.1.tar.gz (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.0/1.0 MB 17.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting googledrivedownloader (from depccg)\n",
      "  Downloading googledrivedownloader-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from depccg) (2.32.5)\n",
      "Collecting allennlp (from depccg)\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting allennlp-models (from depccg)\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting janome (from depccg)\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (80.9.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (4.25.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\anaconda3\\lib\\site-packages (from chainer>=2.0.0->depccg) (1.17.0)\n",
      "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting allennlp (from depccg)\n",
      "  Downloading allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from allennlp->depccg) (2.8.0)\n",
      "Requirement already satisfied: overrides in c:\\anaconda3\\lib\\site-packages (from allennlp->depccg) (7.4.0)\n",
      "Collecting spacy<2.2,>=2.1.0 (from allennlp->depccg)\n",
      "  Downloading spacy-2.1.9.tar.gz (30.7 MB)\n",
      "     ---------------------------------------- 0.0/30.7 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 3.1/30.7 MB 45.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 5.2/30.7 MB 17.7 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 5.2/30.7 MB 17.7 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 5.2/30.7 MB 17.7 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 5.2/30.7 MB 17.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 7.3/30.7 MB 6.2 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 7.3/30.7 MB 6.2 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 7.3/30.7 MB 6.2 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 8.4/30.7 MB 4.6 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 8.4/30.7 MB 4.6 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 9.4/30.7 MB 4.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 16.0/30.7 MB 6.4 MB/s eta 0:00:03\n",
      "     ------------------------------------- - 29.4/30.7 MB 10.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 30.7/30.7 MB 10.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [35 lines of output]\n",
      "  Collecting setuptools\n",
      "    Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Collecting wheel<0.33.0,>0.32.0\n",
      "    Downloading wheel-0.32.3-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "  Collecting Cython\n",
      "    Using cached cython-3.1.3-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "  Collecting cymem<2.1.0,>=2.0.2\n",
      "    Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "  Collecting preshed<2.1.0,>=2.0.1\n",
      "    Downloading preshed-2.0.1.tar.gz (113 kB)\n",
      "    Preparing metadata (setup.py): started\n",
      "    Preparing metadata (setup.py): finished with status 'error'\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    python setup.py egg_info did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [6 lines of output]\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 2, in <module>\n",
      "      File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "      File \"C:\\Users\\itsge\\AppData\\Local\\Temp\\pip-install-i_vp51k2\\preshed_06ff2bb4d63744d4852cef5da10ac174\\setup.py\", line 9, in <module>\n",
      "        from distutils import ccompiler, msvccompiler\n",
      "    ImportError: cannot import name 'msvccompiler' from 'distutils' (C:\\anaconda3\\Lib\\site-packages\\setuptools\\_distutils\\__init__.py). Did you mean: 'ccompiler'?\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: metadata-generation-failed\n",
      "  \n",
      "  Encountered error while generating package metadata.\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This is an issue with the package mentioned above, not pip.\n",
      "  hint: See above for details.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install depccg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b0f18-4f62-4340-8e98-3d235c88c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name qnlp_env python=3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2358c3-a30e-472f-b69d-8ddaff54568f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
